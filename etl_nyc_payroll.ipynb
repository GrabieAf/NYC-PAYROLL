{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the neccessary libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@KUNCE_AF:50423\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, trim, to_date\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize a Spark session\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNYC Payroll ETL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@KUNCE_AF:50423\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import pandas as pd \n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql.functions import lit\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, to_date\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Payroll ETL\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@KUNCE_AF:50583\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context:\n\u001b[0;32m      6\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m----> 8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNYC Payroll ETL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@KUNCE_AF:50583\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing session\n",
    "from pyspark import SparkContext\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Payroll ETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.149:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYC Payroll ETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2062a3a93d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and convert the cvs into dataframe using spark\n",
    "AgencyMaster_df = spark.read.csv(r'C:\\Users\\abiol\\OneDrive\\Desktop\\10ANALYTICS DATA ENGINEERING\\NYC PAYROLL PROJECT\\NYC-PAYROLL\\datasets\\AgencyMaster.csv', header=True, inferSchema=True)\n",
    "EmpMaster_df = spark.read.csv(r'datasets\\EmpMaster.csv', header=True, inferSchema=True)\n",
    "TitleMaster_df = spark.read.csv(r'datasets\\TitleMaster.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|AgencyID|          AgencyName|\n",
      "+--------+--------------------+\n",
      "|    2001|ADMIN FOR CHILDRE...|\n",
      "|    2002|ADMIN TRIALS AND ...|\n",
      "|    2003| BOARD OF CORRECTION|\n",
      "|    2004|   BOARD OF ELECTION|\n",
      "|    2005|BOARD OF ELECTION...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+---------+\n",
      "|EmployeeID|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|    100001|  AACHEN|    DAVID|\n",
      "|    100002|  AACHEN|   MONICA|\n",
      "|    100003|  AADAMS|  LAMMELL|\n",
      "|    100004|   AADIL|     IRIS|\n",
      "|    100005|  AALAAM|     AMIR|\n",
      "+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|TitleCode|    TitleDescription|\n",
      "+---------+--------------------+\n",
      "|    40001|*ADM SCHOOL SECUR...|\n",
      "|    40002|*ADMIN SCHL SECUR...|\n",
      "|    40003|    *AGENCY ATTORNEY|\n",
      "|    40004|*ASSISTANT ADVOCA...|\n",
      "|    40005|*ASSOCIATE EDUCAT...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AgencyMaster_df.show(5)\n",
    "EmpMaster_df.show(5)\n",
    "TitleMaster_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AgencyID: integer (nullable = true)\n",
      " |-- AgencyName: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- TitleCode: integer (nullable = true)\n",
      " |-- TitleDescription: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for the right datatype for each column in various dataframes\n",
    "AgencyMaster_df.printSchema()\n",
    "EmpMaster_df.printSchema()\n",
    "TitleMaster_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|AgencyID|AgencyName|\n",
      "+--------+----------+\n",
      "|       0|         0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values in the dataframe\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "AgencyMaster_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in AgencyMaster_df.columns]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|EmployeeID|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|         0|       0|        0|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values in the dataframe\n",
    "EmpMaster_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in EmpMaster_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|TitleCode|TitleDescription|\n",
      "+---------+----------------+\n",
      "|        0|               1|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values in the dataframe\n",
    "TitleMaster_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in TitleMaster_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the column with missing values\n",
    "TitleMaster_df = TitleMaster_df.dropna(subset=[\"TitleDescription\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|TitleCode|TitleDescription|\n",
      "+---------+----------------+\n",
      "|        0|               0|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TitleMaster_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in TitleMaster_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and convert the cvs into dataframe using spark\n",
    "nycpayroll2020_df = spark.read.csv(r'datasets\\nycpayroll_2020.csv', header=True, inferSchema=True)\n",
    "nycpayroll2021_df = spark.read.csv(r'datasets\\nycpayroll_2021.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------------------+----------+----------+---------+---------------+-------------------+---------+--------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "|FiscalYear|PayrollNumber|AgencyID|          AgencyName|EmployeeID|  LastName|FirstName|AgencyStartDate|WorkLocationBorough|TitleCode|    TitleDescription|LeaveStatusasofJune30|BaseSalary| PayBasis|RegularHours|RegularGrossPaid|OTHours|TotalOTPaid|TotalOtherPay|\n",
      "+----------+-------------+--------+--------------------+----------+----------+---------+---------------+-------------------+---------+--------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "|      2020|           17|    2120|OFFICE OF EMERGEN...|     10001|    GEAGER| VERONICA|      9/12/2016|           BROOKLYN|    40447|EMERGENCY PREPARE...|               ACTIVE|   86005.0|per Annum|      1820.0|        84698.21|    0.0|        0.0|          0.0|\n",
      "|      2020|           17|    2120|OFFICE OF EMERGEN...|    149612|     ROTTA| JONATHAN|      9/16/2013|           BROOKLYN|    40447|EMERGENCY PREPARE...|               ACTIVE|   86005.0|per Annum|      1820.0|        84698.21|    0.0|        0.0|          0.0|\n",
      "|      2020|           17|    2120|OFFICE OF EMERGEN...|    206583| WILSON II|   ROBERT|      4/30/2018|           BROOKLYN|    40447|EMERGENCY PREPARE...|               ACTIVE|   86005.0|per Annum|      1820.0|        84698.21|    0.0|        0.0|          0.0|\n",
      "|      2020|           17|    2120|OFFICE OF EMERGEN...|    199874|WASHINGTON|   MORIAH|      3/18/2019|           BROOKLYN|    40447|EMERGENCY PREPARE...|               ACTIVE|   86005.0|per Annum|      1820.0|        87900.95|    0.0|        0.0|     -3202.74|\n",
      "|      2020|           17|    2120|OFFICE OF EMERGEN...|     58036|  KRAWCZYK|   AMANDA|      5/15/2017|           BROOKLYN|    40447|EMERGENCY PREPARE...|               ACTIVE|   86005.0|per Annum|      1820.0|        83976.54|    0.0|        0.0|          0.0|\n",
      "+----------+-------------+--------+--------------------+----------+----------+---------+---------------+-------------------+---------+--------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------+----------+--------------------+----------+-----------+---------+---------------+-------------------+---------+-------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "|FiscalYear|PayrollNumber|AgencyCode|          AgencyName|EmployeeID|   LastName|FirstName|AgencyStartDate|WorkLocationBorough|TitleCode|   TitleDescription|LeaveStatusasofJune30|BaseSalary| PayBasis|RegularHours|RegularGrossPaid|OTHours|TotalOTPaid|TotalOtherPay|\n",
      "+----------+-------------+----------+--------------------+----------+-----------+---------+---------------+-------------------+---------+-------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "|      2021|          996|      2153|NYC HOUSING AUTHO...|    209184|MUSTACIUOLO|     VITO|      2/26/2018|          MANHATTAN|    40475| EXECUTIVE DIRECTOR|               ACTIVE|  258000.0|per Annum|        1820|        257260.3|    0.0|        0.0|     258000.0|\n",
      "|      2021|          996|      2153|NYC HOUSING AUTHO...|    302330|       RUSS|  GREGORY|      8/12/2019|          MANHATTAN|    41143|              CHAIR|               ACTIVE|  414707.0|per Annum|        1820|       413518.05|    0.0|        0.0|        500.0|\n",
      "|      2021|          816|      2129|DEPT OF HEALTH/ME...|     49788|   HALLAHAN|  PATRICK|      2/26/2018|           BROOKLYN|    40782|STATIONARY ENGINEER|               ACTIVE|     508.8|  per Day|        2080|        132288.0|2115.25|  218628.18|     56616.07|\n",
      "|      2021|          816|      2129|DEPT OF HEALTH/ME...|    251626|     PETTIT|  PATRICK|       8/2/2010|          MANHATTAN|    40782|STATIONARY ENGINEER|               ACTIVE|     508.8|  per Day|        2080|        132288.0|2152.75|  218694.96|     38611.82|\n",
      "|      2021|          816|      2129|DEPT OF HEALTH/ME...|    364376|   TELEHANY|  STEPHEN|      1/16/2007|             QUEENS|    40782|STATIONARY ENGINEER|               ACTIVE|     508.8|  per Day|        2080|        132288.0|1876.25|  192296.19|      51160.2|\n",
      "+----------+-------------+----------+--------------------+----------+-----------+---------+---------------+-------------------+---------+-------------------+---------------------+----------+---------+------------+----------------+-------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycpayroll2020_df.show(5)\n",
    "nycpayroll2021_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns do not match! Fixing column differences...\n",
      "Extra columns in 2020: {'agencyid'}\n",
      "Extra columns in 2021: {'agencycode'}\n"
     ]
    }
   ],
   "source": [
    "# Convert column names to lowercase and remove extra spaces\n",
    "nycpayroll2020_df = nycpayroll2020_df.toDF(*[col.lower().strip() for col in nycpayroll2020_df.columns])\n",
    "nycpayroll2021_df = nycpayroll2021_df.toDF(*[col.lower().strip() for col in nycpayroll2021_df.columns])\n",
    "\n",
    "# Check if columns match\n",
    "payroll2020 = set(nycpayroll2020_df.columns)\n",
    "payroll2021 = set(nycpayroll2021_df.columns)\n",
    "\n",
    "if payroll2020 != payroll2021:\n",
    "    print(\"Columns do not match! Fixing column differences...\")\n",
    "    print(\"Extra columns in 2020:\", payroll2020 - payroll2021)\n",
    "    print(\"Extra columns in 2021:\", payroll2021 - payroll2020)\n",
    "else:\n",
    "    print(\"Columns match, ready to merge.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above function shows there are missing columns in each dataset payroll2020 & payroll2021, We will proceed to merge both dataset into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+-------------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|agencycode|agencyid|          agencyname|agencystartdate|basesalary|employeeid|firstname|fiscalyear|     lastname|leavestatusasofjune30|othours| paybasis|payrollnumber|regulargrosspaid|regularhours|titlecode|    titledescription|totalotherpay|totalotpaid|worklocationborough|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+-------------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      9/12/2016|   86005.0|     10001| VERONICA|      2020|       GEAGER|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      9/16/2013|   86005.0|    149612| JONATHAN|      2020|        ROTTA|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      4/30/2018|   86005.0|    206583|   ROBERT|      2020|    WILSON II|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      3/18/2019|   86005.0|    199874|   MORIAH|      2020|   WASHINGTON|               ACTIVE|    0.0|per Annum|           17|        87900.95|      1820.0|    40447|EMERGENCY PREPARE...|     -3202.74|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      5/15/2017|   86005.0|     58036|   AMANDA|      2020|     KRAWCZYK|               ACTIVE|    0.0|per Annum|           17|        83976.54|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      12/1/2014|   86005.0|    104393|  JALEESA|      2020|      MURRELL|               ACTIVE|    0.0|per Annum|           17|        83877.36|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      6/30/2014|  236088.0|    186781|   JOSEPH|      2020|     ESPOSITO|               CEASED|    0.0|per Annum|           17|         2716.38|        21.0|    40285|COMMISSIONER OF E...|     80141.89|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      6/27/2016|   86005.0|    149700|    CAROL|      2020|        CHANG|               ACTIVE|   15.0|per Annum|           17|        81887.99|      1820.0|    40447|EMERGENCY PREPARE...|        29.97|     679.33|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|     11/20/2017|   83791.0|    121565|  MICHAEL|      2020|    BEILINSON|               ACTIVE|    0.0|per Annum|           17|        82517.67|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|       1/4/2016|   83791.0|    141167|     TODD|      2020|    CALLENDER|               ACTIVE|    0.0|per Annum|           17|        82517.67|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|       4/6/2015|   73049.0|    123132|   JUSTIN|      2020|      BENNETT|               ACTIVE| 175.25|per Annum|           17|        71938.89|      1820.0|    40448|EMERGENCY PREPARE...|        120.1|    8786.47|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|     10/10/2017|   66603.0|     97057|  CHERISE|      2020|     MOHAMMED|               ACTIVE|  352.0|per Annum|           17|        64110.28|      1820.0|    40448|EMERGENCY PREPARE...|        26.71|   16667.92|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|     11/26/2018|   68751.0|    155154| SAMANTHA|      2020|     CLEMENTS|               ACTIVE|  237.0|per Annum|           17|        67706.51|      1820.0|    40448|EMERGENCY PREPARE...|       566.13|   11332.32|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      9/27/2010|   71231.0|    115541|  JILLIAN|      2020|OSORIO-SUAREZ|               ACTIVE|  32.25|per Annum|           17|         70148.5|      1820.0|    40448|EMERGENCY PREPARE...|      6860.54|    1314.22|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|       1/3/2017|   73317.0|     96376|  MATTHEW|      2020|     MITCHELL|               ACTIVE|  113.5|per Annum|           17|        72203.21|      1820.0|    40448|EMERGENCY PREPARE...|       458.37|    5643.55|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|       3/4/2019|   67980.0|    143597|    KAYLI|      2020|      CAPSHAW|               ACTIVE|  216.0|per Annum|           17|        66947.11|      1820.0|    40448|EMERGENCY PREPARE...|        469.3|   10630.76|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|     12/10/2018|   73049.0|    181023|  AMINATA|      2020|      DUMBUYA|               ACTIVE|  127.0|per Annum|           17|        70352.18|      1820.0|    40448|EMERGENCY PREPARE...|       431.81|    6574.12|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|       3/5/2018|   59083.0|    152023|  MICHAEL|      2020|  CHIRICHELLA|               ACTIVE|  395.5|per Annum|           17|        58185.25|      1820.0|    40448|EMERGENCY PREPARE...|       252.14|   17255.06|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      9/12/2016|   66603.0|    158677| BRITTANY|      2020|     SCHILIRO|               ACTIVE| 199.25|per Annum|           17|        65591.04|      1820.0|    40448|EMERGENCY PREPARE...|        324.4|    9713.85|           BROOKLYN|\n",
      "|      NULL|    2120|OFFICE OF EMERGEN...|      5/28/2017|   66603.0|    135408|  TASHAWN|      2020|        BROWN|               ACTIVE|  192.0|per Annum|           17|        65591.04|      1820.0|    40448|EMERGENCY PREPARE...|        769.6|    8757.41|           BROOKLYN|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+-------------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Merge DataFrames\").getOrCreate()\n",
    "\n",
    "# Standardizing column names (convert to lowercase and remove spaces)\n",
    "nycpayroll2020_df = nycpayroll2020_df.toDF(*[col.lower().strip() for col in nycpayroll2020_df.columns])\n",
    "nycpayroll2021_df = nycpayroll2021_df.toDF(*[col.lower().strip() for col in nycpayroll2021_df.columns])\n",
    "\n",
    "# Get column sets for each DataFrame\n",
    "payroll2020 = set(nycpayroll2020_df.columns)\n",
    "payroll2021 = set(nycpayroll2021_df.columns)\n",
    "\n",
    "# Identify extra columns in each dataset\n",
    "# Columns in 2020 but not in 2021\n",
    "extra_2020columns = list(payroll2020 - payroll2021) \n",
    "    \n",
    "# Columns in 2021 but not in 2020\n",
    "extra_2021columns = list(payroll2021 - payroll2020)  \n",
    "\n",
    "# Add missing columns with NULL values\n",
    "for col_name in extra_2020columns:\n",
    "    nycpayroll2021_df = nycpayroll2021_df.withColumn(col_name, lit(None))  \n",
    "\n",
    "for col_name in extra_2021columns:\n",
    "    nycpayroll2020_df = nycpayroll2020_df.withColumn(col_name, lit(None))  \n",
    "\n",
    "# Ensure both DataFrames have the same column order\n",
    "common_columns = sorted(payroll2020.union(payroll2021))  \n",
    "nycpayroll2020_df = nycpayroll2020_df.select(common_columns)  \n",
    "nycpayroll2021_df = nycpayroll2021_df.select(common_columns)  \n",
    "\n",
    "# Merge both DataFrames\n",
    "mergedpayroll_df = nycpayroll2020_df.unionByName(nycpayroll2021_df)\n",
    "\n",
    "# Show merged dataset\n",
    "mergedpayroll_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns are: 20\n",
      "The number of rows are: 201\n",
      "The shape of the dataframe mergedpayroll_df: (201, 20)\n"
     ]
    }
   ],
   "source": [
    "mergedpayroll_df.columns\n",
    "\n",
    "#number of columns\n",
    "num_cols = len(mergedpayroll_df.columns)\n",
    "print('The number of columns are:', num_cols)\n",
    "\n",
    "#number of rows\n",
    "num_rows = mergedpayroll_df.count()\n",
    "\n",
    "print('The number of rows are:', num_rows)\n",
    "\n",
    "print(f'The shape of the dataframe mergedpayroll_df: ({num_rows}, {num_cols})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agencycode: integer (nullable = true)\n",
      " |-- agencyid: integer (nullable = true)\n",
      " |-- agencyname: string (nullable = true)\n",
      " |-- agencystartdate: string (nullable = true)\n",
      " |-- basesalary: double (nullable = true)\n",
      " |-- employeeid: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- fiscalyear: integer (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- leavestatusasofjune30: string (nullable = true)\n",
      " |-- othours: double (nullable = true)\n",
      " |-- paybasis: string (nullable = true)\n",
      " |-- payrollnumber: integer (nullable = true)\n",
      " |-- regulargrosspaid: double (nullable = true)\n",
      " |-- regularhours: double (nullable = true)\n",
      " |-- titlecode: integer (nullable = true)\n",
      " |-- titledescription: string (nullable = true)\n",
      " |-- totalotherpay: double (nullable = true)\n",
      " |-- totalotpaid: double (nullable = true)\n",
      " |-- worklocationborough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedpayroll_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agencycode Nulls 100\n",
      "agencyid Nulls 101\n",
      "agencyname Nulls 0\n",
      "agencystartdate Nulls 0\n",
      "basesalary Nulls 0\n",
      "employeeid Nulls 0\n",
      "firstname Nulls 0\n",
      "fiscalyear Nulls 0\n",
      "lastname Nulls 0\n",
      "leavestatusasofjune30 Nulls 0\n",
      "othours Nulls 0\n",
      "paybasis Nulls 0\n",
      "payrollnumber Nulls 0\n",
      "regulargrosspaid Nulls 0\n",
      "regularhours Nulls 0\n",
      "titlecode Nulls 0\n",
      "titledescription Nulls 0\n",
      "totalotherpay Nulls 0\n",
      "totalotpaid Nulls 0\n",
      "worklocationborough Nulls 0\n"
     ]
    }
   ],
   "source": [
    "#Check and count for null values\n",
    "for column in mergedpayroll_df.columns:\n",
    "    print(column, 'Nulls', mergedpayroll_df.filter(mergedpayroll_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the null values\n",
    "mergedpayroll_df = mergedpayroll_df.fillna(\n",
    "    {\"agencycode\": 0, \n",
    "     \"agencyid\": 0\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agencycode Nulls 0\n",
      "agencyid Nulls 0\n",
      "agencyname Nulls 0\n",
      "agencystartdate Nulls 0\n",
      "basesalary Nulls 0\n",
      "employeeid Nulls 0\n",
      "firstname Nulls 0\n",
      "fiscalyear Nulls 0\n",
      "lastname Nulls 0\n",
      "leavestatusasofjune30 Nulls 0\n",
      "othours Nulls 0\n",
      "paybasis Nulls 0\n",
      "payrollnumber Nulls 0\n",
      "regulargrosspaid Nulls 0\n",
      "regularhours Nulls 0\n",
      "titlecode Nulls 0\n",
      "titledescription Nulls 0\n",
      "totalotherpay Nulls 0\n",
      "totalotpaid Nulls 0\n",
      "worklocationborough Nulls 0\n"
     ]
    }
   ],
   "source": [
    "#confirming no null values\n",
    "for column in mergedpayroll_df.columns:\n",
    "    print(column, 'Nulls', mergedpayroll_df.filter(mergedpayroll_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking and removing duplicate values\n",
    "mergedpayroll_df = mergedpayroll_df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agencycode: integer (nullable = false)\n",
      " |-- agencyid: integer (nullable = false)\n",
      " |-- agencyname: string (nullable = true)\n",
      " |-- agencystartdate: date (nullable = true)\n",
      " |-- basesalary: integer (nullable = true)\n",
      " |-- employeeid: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- fiscalyear: integer (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- leavestatusasofjune30: string (nullable = true)\n",
      " |-- othours: double (nullable = true)\n",
      " |-- paybasis: string (nullable = true)\n",
      " |-- payrollnumber: integer (nullable = true)\n",
      " |-- regulargrosspaid: double (nullable = true)\n",
      " |-- regularhours: double (nullable = true)\n",
      " |-- titlecode: integer (nullable = true)\n",
      " |-- titledescription: string (nullable = true)\n",
      " |-- totalotherpay: double (nullable = true)\n",
      " |-- totalotpaid: double (nullable = true)\n",
      " |-- worklocationborough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to proper format (replace 'date_column' with actual column)\n",
    "mergedpayroll_df = mergedpayroll_df.withColumn(\"agencystartdate\", to_date(col(\"agencystartdate\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "#convert columns to interger from strings\n",
    "mergedpayroll_df = mergedpayroll_df.withColumn(\"basesalary\", col(\"basesalary\").cast(\"int\"))\n",
    "mergedpayroll_df = mergedpayroll_df.withColumn(\"agencycode\", col(\"agencycode\").cast(\"int\"))\n",
    "mergedpayroll_df = mergedpayroll_df.withColumn(\"agencyid\", col(\"agencyid\").cast(\"int\"))\n",
    "mergedpayroll_df = mergedpayroll_df.withColumn(\"employeeid\", col(\"employeeid\").cast(\"int\"))\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert string columns to numeric types\n",
    "mergedpayroll_df = (\n",
    "    mergedpayroll_df\n",
    "    .withColumn(\"fiscalyear\", col(\"fiscalyear\").cast(\"int\"))\n",
    "    .withColumn(\"othours\", col(\"othours\").cast(\"double\"))\n",
    "    .withColumn(\"payrollnumber\", col(\"payrollnumber\").cast(\"int\"))\n",
    "    .withColumn(\"regulargrosspaid\", col(\"regulargrosspaid\").cast(\"double\"))\n",
    "    .withColumn(\"regularhours\", col(\"regularhours\").cast(\"double\"))\n",
    "    .withColumn(\"titlecode\", col(\"titlecode\").cast(\"int\"))\n",
    "    .withColumn(\"totalotherpay\", col(\"totalotherpay\").cast(\"double\"))\n",
    "    .withColumn(\"totalotpaid\", col(\"totalotpaid\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# Show schema after changes\n",
    "mergedpayroll_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|agencycode|agencyid|          agencyname|agencystartdate|basesalary|employeeid|firstname|fiscalyear| lastname|leavestatusasofjune30|othours| paybasis|payrollnumber|regulargrosspaid|regularhours|titlecode|    titledescription|totalotherpay|totalotpaid|worklocationborough|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2013-09-16|     86005|    149612| JONATHAN|      2020|    ROTTA|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2015-04-06|     73049|    123132|   JUSTIN|      2020|  BENNETT|               ACTIVE| 175.25|per Annum|           17|        71938.89|      1820.0|    40448|EMERGENCY PREPARE...|        120.1|    8786.47|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2020-04-13|     75197|    176308|  ANTONIO|      2020|  DIMARCO|               ACTIVE|  204.5|per Annum|           17|        14381.95|       350.0|    40448|EMERGENCY PREPARE...|         14.4|   10917.33|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2016-01-04|     83791|    141167|     TODD|      2020|CALLENDER|               ACTIVE|    0.0|per Annum|           17|        82517.67|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2017-05-15|     86005|     58036|   AMANDA|      2020| KRAWCZYK|               ACTIVE|    0.0|per Annum|           17|        83976.54|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "mergedpayroll_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agencycode: integer (nullable = false)\n",
      " |-- agencyid: integer (nullable = false)\n",
      " |-- agencyname: string (nullable = true)\n",
      " |-- agencystartdate: date (nullable = true)\n",
      " |-- basesalary: integer (nullable = true)\n",
      " |-- employeeid: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- fiscalyear: integer (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- leavestatusasofjune30: string (nullable = true)\n",
      " |-- othours: double (nullable = true)\n",
      " |-- paybasis: string (nullable = true)\n",
      " |-- payrollnumber: integer (nullable = true)\n",
      " |-- regulargrosspaid: double (nullable = true)\n",
      " |-- regularhours: double (nullable = true)\n",
      " |-- titlecode: integer (nullable = true)\n",
      " |-- titledescription: string (nullable = true)\n",
      " |-- totalotherpay: double (nullable = true)\n",
      " |-- totalotpaid: double (nullable = true)\n",
      " |-- worklocationborough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedpayroll_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|agencycode|agencyid|          agencyname|agencystartdate|basesalary|employeeid|firstname|fiscalyear| lastname|leavestatusasofjune30|othours| paybasis|payrollnumber|regulargrosspaid|regularhours|titlecode|    titledescription|totalotherpay|totalotpaid|worklocationborough|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2013-09-16|     86005|    149612| JONATHAN|      2020|    ROTTA|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2015-04-06|     73049|    123132|   JUSTIN|      2020|  BENNETT|               ACTIVE| 175.25|per Annum|           17|        71938.89|      1820.0|    40448|EMERGENCY PREPARE...|        120.1|    8786.47|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2020-04-13|     75197|    176308|  ANTONIO|      2020|  DIMARCO|               ACTIVE|  204.5|per Annum|           17|        14381.95|       350.0|    40448|EMERGENCY PREPARE...|         14.4|   10917.33|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2016-01-04|     83791|    141167|     TODD|      2020|CALLENDER|               ACTIVE|    0.0|per Annum|           17|        82517.67|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2017-05-15|     86005|     58036|   AMANDA|      2020| KRAWCZYK|               ACTIVE|    0.0|per Annum|           17|        83976.54|      1820.0|    40447|EMERGENCY PREPARE...|          0.0|        0.0|           BROOKLYN|\n",
      "+----------+--------+--------------------+---------------+----------+----------+---------+----------+---------+---------------------+-------+---------+-------------+----------------+------------+---------+--------------------+-------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#datetime parser\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "mergedpayroll_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agencycode',\n",
       " 'agencyid',\n",
       " 'agencyname',\n",
       " 'agencystartdate',\n",
       " 'basesalary',\n",
       " 'employeeid',\n",
       " 'firstname',\n",
       " 'fiscalyear',\n",
       " 'lastname',\n",
       " 'leavestatusasofjune30',\n",
       " 'othours',\n",
       " 'paybasis',\n",
       " 'payrollnumber',\n",
       " 'regulargrosspaid',\n",
       " 'regularhours',\n",
       " 'titlecode',\n",
       " 'titledescription',\n",
       " 'totalotherpay',\n",
       " 'totalotpaid',\n",
       " 'worklocationborough']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedpayroll_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------+\n",
      "|agencycode|agencyid|          agencyname|agencystartdate|\n",
      "+----------+--------+--------------------+---------------+\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2013-09-16|\n",
      "|         0|    2120|OFFICE OF EMERGEN...|     2015-04-06|\n",
      "+----------+--------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------+--------+\n",
      "|employeeid|firstname|lastname|\n",
      "+----------+---------+--------+\n",
      "|    149612| JONATHAN|   ROTTA|\n",
      "|    123132|   JUSTIN| BENNETT|\n",
      "+----------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|titlecode|    titledescription|\n",
      "+---------+--------------------+\n",
      "|    40447|EMERGENCY PREPARE...|\n",
      "|    40448|EMERGENCY PREPARE...|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#agencytable\n",
    "agency_df = mergedpayroll_df.select('agencycode','agencyid','agencyname','agencystartdate')\n",
    "agency_df.show(2)\n",
    "\n",
    "Emp_df = mergedpayroll_df.select('employeeid','firstname','lastname')\n",
    "Emp_df.show(2)\n",
    "\n",
    "Title_df = mergedpayroll_df.select('titlecode','titledescription')\n",
    "Title_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+\n",
      "|title_id|titlecode|    titledescription|\n",
      "+--------+---------+--------------------+\n",
      "|       0|    40447|EMERGENCY PREPARE...|\n",
      "|       1|    40448|EMERGENCY PREPARE...|\n",
      "|       2|    40448|EMERGENCY PREPARE...|\n",
      "|       3|    40447|EMERGENCY PREPARE...|\n",
      "+--------+---------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding the title_id column\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "Title_df = Title_df.withColumn('title_id', monotonically_increasing_id())\n",
    "\n",
    "#rearrange the columns\n",
    "Title_df = Title_df.select('title_id','titlecode','titledescription')\n",
    "\n",
    "Title_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+---------------+\n",
      "|agencyid|agencycode|          agencyname|agencystartdate|\n",
      "+--------+----------+--------------------+---------------+\n",
      "|    2120|         0|OFFICE OF EMERGEN...|     2013-09-16|\n",
      "|    2120|         0|OFFICE OF EMERGEN...|     2015-04-06|\n",
      "+--------+----------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------+--------+\n",
      "|employeeid|firstname|lastname|\n",
      "+----------+---------+--------+\n",
      "|    149612| JONATHAN|   ROTTA|\n",
      "|    123132|   JUSTIN| BENNETT|\n",
      "+----------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+---------+--------------------+\n",
      "|title_id|titlecode|    titledescription|\n",
      "+--------+---------+--------------------+\n",
      "|       0|    40447|EMERGENCY PREPARE...|\n",
      "|       1|    40448|EMERGENCY PREPARE...|\n",
      "+--------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensure all tables are well ordered\n",
    "agency_df = agency_df.select('agencyid','agencycode','agencyname','agencystartdate')\n",
    "agency_df.show(2)\n",
    "\n",
    "Emp_df.show(2)\n",
    "\n",
    "Title_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact table\n",
    "facts_table = mergedpayroll_df.join(agency_df,['agencyid','agencycode','agencyname','agencystartdate'], 'left')\\\n",
    "                             .join(Emp_df,['employeeid','firstname','lastname'], 'left')\\\n",
    "                             .join(Title_df,['titlecode','titledescription'])\\\n",
    "                             .select('agencyid','employeeid','title_id','basesalary','fiscalyear','leavestatusasofjune30', 'othours', 'paybasis', 'payrollnumber',\\\n",
    "                                      'regulargrosspaid','regularhours','totalotherpay','totalotpaid','worklocationborough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------+----------+---------------------+-------+---------+-------------+----------------+------------+-------------+-----------+-------------------+\n",
      "|agencyid|employeeid|title_id|basesalary|fiscalyear|leavestatusasofjune30|othours| paybasis|payrollnumber|regulargrosspaid|regularhours|totalotherpay|totalotpaid|worklocationborough|\n",
      "+--------+----------+--------+----------+----------+---------------------+-------+---------+-------------+----------------+------------+-------------+-----------+-------------------+\n",
      "|    2120|    149612|      92|     86005|      2020|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|          0.0|        0.0|           BROOKLYN|\n",
      "|    2120|    149612|      89|     86005|      2020|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|          0.0|        0.0|           BROOKLYN|\n",
      "|    2120|    149612|      86|     86005|      2020|               ACTIVE|    0.0|per Annum|           17|        84698.21|      1820.0|          0.0|        0.0|           BROOKLYN|\n",
      "+--------+----------+--------+----------+----------+---------------------+-------+---------+-------------+----------------+------------+-------------+-----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "facts_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agency_pd_df = agency_df.toPandas()\n",
    "Emp_pd_df = Emp_df.toPandas()\n",
    "Title_pd_df = Title_df.toPandas()\n",
    "facts_table_pd_df = facts_table.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   agencyid  agencycode                      agencyname agencystartdate\n",
      "0      2120           0  OFFICE OF EMERGENCY MANAGEMENT      2013-09-16\n",
      "1      2120           0  OFFICE OF EMERGENCY MANAGEMENT      2015-04-06\n",
      "   employeeid firstname lastname\n",
      "0      149612  JONATHAN    ROTTA\n",
      "1      123132    JUSTIN  BENNETT\n",
      "   title_id  titlecode                   titledescription\n",
      "0         0      40447     EMERGENCY PREPAREDNESS MANAGER\n",
      "1         1      40448  EMERGENCY PREPAREDNESS SPECIALIST\n",
      "   agencyid  employeeid  title_id  basesalary  fiscalyear  \\\n",
      "0      2120      149612        92       86005        2020   \n",
      "1      2120      149612        89       86005        2020   \n",
      "\n",
      "  leavestatusasofjune30  othours   paybasis  payrollnumber  regulargrosspaid  \\\n",
      "0                ACTIVE      0.0  per Annum             17          84698.21   \n",
      "1                ACTIVE      0.0  per Annum             17          84698.21   \n",
      "\n",
      "   regularhours  totalotherpay  totalotpaid worklocationborough  \n",
      "0        1820.0            0.0          0.0            BROOKLYN  \n",
      "1        1820.0            0.0          0.0            BROOKLYN  \n"
     ]
    }
   ],
   "source": [
    "print(Agency_pd_df.head(2))\n",
    "print(Emp_pd_df.head(2))\n",
    "print(Title_pd_df.head(2))\n",
    "print(facts_table_pd_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'type' has no attribute 'Agency_pd_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAgency_pd_df\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'type' has no attribute 'Agency_pd_df'"
     ]
    }
   ],
   "source": [
    "print(type.Agency_pd_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary sqlalchemy apache-airflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\users\\abiol\\.conda\\envs\\nycpayrollenv\\lib\\site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import pandas as pd\n",
    "\n",
    "# Define database connection parameters\n",
    "db_params = {\n",
    "    'username': 'postgres',\n",
    "    'password': '030611',  # Ensure this is correct\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'database': 'nycpayroll'\n",
    "}\n",
    "\n",
    "# Use the psycopg2 driver by including it in the URL\n",
    "db_url = f\"postgresql+psycopg2://{db_params['username']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\"\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "if not database_exists(db_url):\n",
    "    create_database(db_url)\n",
    "    print(\"Database created.\")\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(db_url, echo=True)  # Enable echo for debugging\n",
    "\n",
    "# Double-check if DataFrames exist before writing to SQL\n",
    "if 'Agency_pd_df' in locals() and isinstance(Agency_pd_df, pd.DataFrame):\n",
    "    Agency_pd_df.to_sql('agency', con=engine, index=False, if_exists='replace')\n",
    "else:\n",
    "    print(\"Error: Agency_pd_df is not defined or not a DataFrame.\")\n",
    "\n",
    "if 'Emp_pd_df' in locals() and isinstance(Emp_pd_df, pd.DataFrame):\n",
    "    Emp_pd_df.to_sql('employee', con=engine, index=False, if_exists='replace')\n",
    "else:\n",
    "    print(\"Error: Emp_pd_df is not defined or not a DataFrame.\")\n",
    "\n",
    "if 'Title_pd_df' in locals() and isinstance(Title_pd_df, pd.DataFrame):\n",
    "    Title_pd_df.to_sql('title', con=engine, index=False, if_exists='replace')\n",
    "else:\n",
    "    print(\"Error: Title_pd_df is not defined or not a DataFrame.\")\n",
    "\n",
    "if 'facts_table_pd_df' in locals() and isinstance(facts_table_pd_df, pd.DataFrame):\n",
    "    facts_table_pd_df.to_sql('fact_table', con=engine, index=False, if_exists='replace')\n",
    "else:\n",
    "    print(\"Error: facts_table_pd_df is not defined or not a DataFrame.\")\n",
    "\n",
    "print('Database, tables, and data loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.54\n",
      "2.9.10 (dt dec pq3 ext lo64)\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "print(sqlalchemy.__version__)  # Should print version (e.g., '1.4.39' or higher)\n",
    "print(psycopg2.__version__)  # Should print version (e.g., '2.9.3' or higher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SQLAlchemy psycopg2 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully connected to the database.\n",
      " Error occurred: 'Engine' object has no attribute 'cursor'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiol\\AppData\\Local\\Temp\\ipykernel_19652\\3717063597.py:36: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df.to_sql(table_name, con=engine, index=False, if_exists='replace', method='multi')\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'username': 'postgres',\n",
    "    'password': '030611',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'database': 'nycpayroll'\n",
    "}\n",
    "\n",
    "# Use the correct connection string (adding `psycopg2`)\n",
    "db_url = f\"postgresql+psycopg2://{db_params['username']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(db_url)\n",
    "    print(\" Successfully connected to the database.\")\n",
    "except Exception as e:\n",
    "    print(f\" Connection Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure DataFrames exist\n",
    "dataframes = {\n",
    "    'agency': Agency_pd_df,\n",
    "    'employee': Emp_pd_df,\n",
    "    'title': Title_pd_df,\n",
    "    'fact_table': facts_table_pd_df\n",
    "}\n",
    "\n",
    "# Use engine directly instead of connection\n",
    "try:\n",
    "    for table_name, df in dataframes.items():\n",
    "        if isinstance(df, pd.DataFrame):  # Ensure it's a DataFrame\n",
    "            df.to_sql(table_name, con=engine, index=False, if_exists='replace', method='multi')\n",
    "            print(f\" Loaded {table_name} successfully.\")\n",
    "        else:\n",
    "            print(f\" Warning: {table_name} is not a DataFrame.\")\n",
    "\n",
    "    print(' All tables loaded successfully.')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nycpayrollenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
